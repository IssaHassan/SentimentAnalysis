{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-cnn-word2vec-feb-27.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lkgp6rhb9Q6j",
        "colab_type": "code",
        "outputId": "3de760fc-6f6a-4f99-d87c-c5f5be8e30ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRs3l_uqKkMM",
        "colab_type": "code",
        "outputId": "be947023-c489-4cd1-b48c-a0cb922673be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/rnn/data/28500Hersh-Stanford-Airline.csv', encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
        "                        usecols=[0,1])\n",
        "data.columns = ['SentimentText', 'Sentiment']\n",
        "\n",
        "# remove neutral for now\n",
        "data = data[(data['Sentiment'] != 2)]\n",
        "\n",
        "# Replace 4 with 1 (for positive value)\n",
        "data['Sentiment'] = data['Sentiment'].map(lambda s: 1 if s == 4 else 0)\n",
        "\n",
        "data.head(20)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Its a restricted area, and inhospitablein ce...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The M.E.K. had its beginnings as a Marxist-Isl...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>But, within a few years, the group was waging ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The M.E.K.s ties with Western intelligence de...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Funds were covertly passed to a number of diss...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Despite the growing ties, and a much-intensifi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>He also was told, he said, that the men doing ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>It was the ad-hoc training that provoked the w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>I told one of the guys who called me that the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>The Iranians are very, very good at counterint...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Allan Gerson, a Washington attorney for the M....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>But such training, if true, he said, would be ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>How can the U.S. train those on States foreig...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>I said Id get back to them, but never did.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>In a telephone interview, he acknowledged that...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Khodabandeh said that he had been with the gro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>For the past decade, he and his English wife h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Five Iranian nuclear scientists have been assa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>M.E.K. spokesmen have denied any involvement i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>He said that the targets were not Einsteins;...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        SentimentText  Sentiment\n",
              "1   Its a restricted area, and inhospitablein ce...          0\n",
              "3   The M.E.K. had its beginnings as a Marxist-Isl...          0\n",
              "5   But, within a few years, the group was waging ...          0\n",
              "8   The M.E.K.s ties with Western intelligence de...          0\n",
              "9   Funds were covertly passed to a number of diss...          0\n",
              "12  Despite the growing ties, and a much-intensifi...          0\n",
              "19  He also was told, he said, that the men doing ...          0\n",
              "21  It was the ad-hoc training that provoked the w...          0\n",
              "22  I told one of the guys who called me that the...          0\n",
              "23  The Iranians are very, very good at counterint...          0\n",
              "26  Allan Gerson, a Washington attorney for the M....          1\n",
              "28  But such training, if true, he said, would be ...          0\n",
              "29  How can the U.S. train those on States foreig...          0\n",
              "33        I said Id get back to them, but never did.          0\n",
              "36  In a telephone interview, he acknowledged that...          0\n",
              "37  Khodabandeh said that he had been with the gro...          1\n",
              "38  For the past decade, he and his English wife h...          1\n",
              "44  Five Iranian nuclear scientists have been assa...          0\n",
              "45  M.E.K. spokesmen have denied any involvement i...          0\n",
              "48  He said that the targets were not Einsteins;...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "3R7OeLti-vqk",
        "colab_type": "code",
        "outputId": "49590828-9cb0-4c56-ecb7-9825db89367b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "appos = {\n",
        "  \"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\",\n",
        "  \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
        "  \"haven't\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"i would\",\n",
        "  \"i'd\" : \"i had\", \"i'll\" : \"i will\", \"i'm\" : \"i am\", \"isn't\" : \"is not\", \"it's\" : \"it is\", \"it'll\":\"it will\",\n",
        "  \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\",\n",
        "  \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\",\n",
        "  \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\",\n",
        "  \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \"what'll\" : \"what will\",\n",
        "  \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\",\n",
        "  \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\",\n",
        "  \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\",\n",
        "  \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "  # Remove whitespaces and make strings lowercase\n",
        "  text = text.strip().lower()\n",
        "  words = text.split()\n",
        "  # Nagation handling\n",
        "  reformed = [appos[word] if word in appos else word for word in words]\n",
        "  text = \" \".join(reformed)\n",
        "  pattern = '(@(\\w+))'                # usermention (@username)\n",
        "  pattern += '|(#(\\w+))'              # hashtags (#somehashtag)\n",
        "  pattern += '|([^\\w\\s])'             # emojis 😀\n",
        "  pattern += '|(\\\\w+:\\\\/\\\\/\\\\S+)'     # urls (https://google.com)\n",
        "  pattern += '|(\\d+)'                 # numbers\n",
        "  text = ' '.join(re.sub(pattern, ' ', text).split())\n",
        "  return text\n",
        "\n",
        "tqdm.pandas()\n",
        "#data = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/rnn/data/data.csv', error_bad_lines=False)\n",
        "data['CleanText'] = data['SentimentText'].progress_apply(lambda t: clean_text(t))\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19533/19533 [00:00<00:00, 22834.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>CleanText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Its a restricted area, and inhospitablein ce...</td>\n",
              "      <td>0</td>\n",
              "      <td>it s a restricted area and inhospitable in cer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The M.E.K. had its beginnings as a Marxist-Isl...</td>\n",
              "      <td>0</td>\n",
              "      <td>the m e k had its beginnings as a marxist isla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>But, within a few years, the group was waging ...</td>\n",
              "      <td>0</td>\n",
              "      <td>but within a few years the group was waging a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The M.E.K.s ties with Western intelligence de...</td>\n",
              "      <td>0</td>\n",
              "      <td>the m e k s ties with western intelligence dee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Funds were covertly passed to a number of diss...</td>\n",
              "      <td>0</td>\n",
              "      <td>funds were covertly passed to a number of diss...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment  \\\n",
              "1  Its a restricted area, and inhospitablein ce...          0   \n",
              "3  The M.E.K. had its beginnings as a Marxist-Isl...          0   \n",
              "5  But, within a few years, the group was waging ...          0   \n",
              "8  The M.E.K.s ties with Western intelligence de...          0   \n",
              "9  Funds were covertly passed to a number of diss...          0   \n",
              "\n",
              "                                           CleanText  \n",
              "1  it s a restricted area and inhospitable in cer...  \n",
              "3  the m e k had its beginnings as a marxist isla...  \n",
              "5  but within a few years the group was waging a ...  \n",
              "8  the m e k s ties with western intelligence dee...  \n",
              "9  funds were covertly passed to a number of diss...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "EeaEY30SGt78",
        "colab_type": "code",
        "outputId": "ece252ab-d6a2-4a73-a75e-4f02cd2135c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data['CleanText'], \n",
        "                                                    data['Sentiment'], \n",
        "                                                    test_size=0.1, \n",
        "                                                    random_state=42,\n",
        "                                                    stratify=data['Sentiment'])\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17579,) (1954,) (17579,) (1954,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMaLiP-X8mBy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "d = {'CleanText': x_test, 'Sentiment': y_test}\n",
        "df = pd.DataFrame(d)\n",
        "df.head(20)\n",
        "df.to_csv('/content/drive/My Drive/SentimentAnalysis/rnn/data/28500HSA-test.csv', index=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9A4gR8rLGjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import multiprocessing\n",
        "from sklearn import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoQvOzgzL-pA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labelize_tweets_ug(tweets, label):\n",
        "  result = []\n",
        "  prefix = label\n",
        "  for i, t in zip(tweets.index, tweets):\n",
        "    result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
        "  return result\n",
        "\n",
        "all_x = pd.concat([x_train, x_test])\n",
        "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f0jCnT65MfuH",
        "colab_type": "code",
        "outputId": "20abb47d-e2be-45df-acc2-0754ddd35f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "# Continuous Bag Of Words\n",
        "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19533/19533 [00:00<00:00, 1772129.96it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "R8TjDALTN-w5",
        "colab_type": "code",
        "outputId": "56257009-c365-4665-bb13-3bf03e16529e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "  model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "  model_ug_cbow.alpha -= 0.002\n",
        "  model_ug_cbow.min_alpha = model_ug_cbow.alpha"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19533/19533 [00:00<00:00, 1688875.28it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1223435.23it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1531552.54it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1904356.94it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1255148.99it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1357401.75it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2096347.07it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1182844.23it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1939797.33it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2119998.45it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1269699.19it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1439493.62it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1921282.77it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1323393.80it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1295007.27it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2104099.96it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1594537.56it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1364273.30it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1496089.19it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1249425.67it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1225191.64it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1943063.75it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1976534.14it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2177586.58it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1351690.95it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1359248.43it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2094524.86it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1345652.15it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1241060.08it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2112999.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 28.8 s, sys: 235 ms, total: 29 s\n",
            "Wall time: 15.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EQQW18vyOM13",
        "colab_type": "code",
        "outputId": "475c698d-01f9-4d65-c2b2-4dd3b3f5b65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Skip Gram\n",
        "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19533/19533 [00:00<00:00, 1270742.96it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XovOngQWSOjE",
        "colab_type": "code",
        "outputId": "8be83c42-8ed6-4d7c-8f0a-495ddabec79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "  model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "  model_ug_sg.alpha -= 0.002\n",
        "  model_ug_sg.min_alpha = model_ug_sg.alpha"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19533/19533 [00:00<00:00, 1153500.04it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1167288.92it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1977631.50it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1688005.36it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1271157.00it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1329708.67it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1251410.46it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2149985.30it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1266401.93it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1428325.81it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1946664.92it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1310879.39it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1931519.71it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1924125.51it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1395054.06it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1284006.83it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2195501.66it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1921147.61it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1272321.72it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1384890.29it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1932111.88it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2139150.89it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1996182.94it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2181529.49it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1935032.48it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1280034.69it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1364273.30it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2202703.13it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 1156822.69it/s]\n",
            "100%|██████████| 19533/19533 [00:00<00:00, 2146717.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 52.6 s, sys: 241 ms, total: 52.9 s\n",
            "Wall time: 28 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sMnViVuRSbJ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_ug_cbow.save('/content/drive/My Drive/SentimentAnalysis/rnn/w2vmodels/w2v_model_ug_cbow.word2vec')\n",
        "model_ug_sg.save('/content/drive/My Drive/SentimentAnalysis/rnn/w2vmodels/w2v_model_ug_sg.word2vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iy87OaQBbhs4",
        "colab_type": "code",
        "outputId": "610ead4c-d551-4352-8aaa-5751ac48c593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8021 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SLzGTGY8bkmg",
        "colab_type": "code",
        "outputId": "6c216245-6540-4875-9571-02fa4ea68334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "MAX_NB_WORDS = 80000\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(data['CleanText'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcq0pYEFcZna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkUXpTJPYwX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# saving tokenizer\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/SentimentAnalysis/rnn/tokenizers/tokenizer-rnn-cnn-w2v-feb-27.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iA0n3By6dCqY",
        "colab_type": "code",
        "outputId": "35cfa079-68f1-4b1e-f5b4-a16e0fa3be21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LENGTH = 35\n",
        "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
        "padded_train_sequences.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17579, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "OlZ4seEWdOag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 200\n",
        "# maximum number of words kept after tokenization based on their word frequency\n",
        "MAX_NB_WORDS = 80000\n",
        "\n",
        "num_words = MAX_NB_WORDS\n",
        "embedding_matrix = np.zeros((num_words, embed_size))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i >= num_words:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9aR9nfPQegC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, GRU, Bidirectional\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Conv1D\n",
        "\n",
        "def get_rnn_cnn_model():\n",
        "  embedding_dim = 200\n",
        "  inp = Input(shape=(MAX_LENGTH, ))\n",
        "  x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
        "  x = SpatialDropout1D(0.3)(x)\n",
        "  x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
        "  x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "  avg_pool = GlobalAveragePooling1D()(x)\n",
        "  max_pool = GlobalMaxPooling1D()(x)\n",
        "  conc = concatenate([avg_pool, max_pool])\n",
        "  outp = Dense(1, activation=\"sigmoid\")(conc)\n",
        "\n",
        "  model = Model(inputs=inp, outputs=outp)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQJz591JfDTq",
        "colab_type": "code",
        "outputId": "8f0a6915-318b-412e-d890-aa7d8714f02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "rnn_cnn_model = get_rnn_cnn_model()\n",
        "\n",
        "filepath=\"/content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-feb-27-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 4\n",
        "\n",
        "history = rnn_cnn_model.fit(x=padded_train_sequences, \n",
        "                    y=y_train, \n",
        "                    validation_data=(padded_test_sequences, y_test), \n",
        "                    batch_size=batch_size, \n",
        "                    callbacks=[checkpoint], \n",
        "                    epochs=epochs, \n",
        "                    verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 17579 samples, validate on 1954 samples\n",
            "Epoch 1/4\n",
            "17579/17579 [==============================] - 16s 923us/step - loss: 0.3820 - acc: 0.8360 - val_loss: 0.2526 - val_acc: 0.8823\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.88229, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-feb-27-01-0.8823.hdf5\n",
            "Epoch 2/4\n",
            "17579/17579 [==============================] - 12s 659us/step - loss: 0.2458 - acc: 0.8966 - val_loss: 0.1505 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.88229 to 0.94012, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-feb-27-02-0.9401.hdf5\n",
            "Epoch 3/4\n",
            "17579/17579 [==============================] - 12s 657us/step - loss: 0.1520 - acc: 0.9416 - val_loss: 0.1204 - val_acc: 0.9560\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.94012 to 0.95599, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-feb-27-03-0.9560.hdf5\n",
            "Epoch 4/4\n",
            "17579/17579 [==============================] - 12s 668us/step - loss: 0.1010 - acc: 0.9619 - val_loss: 0.1125 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.95599 to 0.95701, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-feb-27-04-0.9570.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NQwo9FZv4nUb",
        "colab_type": "code",
        "outputId": "fba3c0ec-789c-4787-9efb-1cd6953caafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>CleanText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Its a restricted area, and inhospitablein ce...</td>\n",
              "      <td>0</td>\n",
              "      <td>it s a restricted area and inhospitable in cer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The M.E.K. had its beginnings as a Marxist-Isl...</td>\n",
              "      <td>0</td>\n",
              "      <td>the m e k had its beginnings as a marxist isla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>But, within a few years, the group was waging ...</td>\n",
              "      <td>0</td>\n",
              "      <td>but within a few years the group was waging a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The M.E.K.s ties with Western intelligence de...</td>\n",
              "      <td>0</td>\n",
              "      <td>the m e k s ties with western intelligence dee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Funds were covertly passed to a number of diss...</td>\n",
              "      <td>0</td>\n",
              "      <td>funds were covertly passed to a number of diss...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment  \\\n",
              "1  Its a restricted area, and inhospitablein ce...          0   \n",
              "3  The M.E.K. had its beginnings as a Marxist-Isl...          0   \n",
              "5  But, within a few years, the group was waging ...          0   \n",
              "8  The M.E.K.s ties with Western intelligence de...          0   \n",
              "9  Funds were covertly passed to a number of diss...          0   \n",
              "\n",
              "                                           CleanText  \n",
              "1  it s a restricted area and inhospitable in cer...  \n",
              "3  the m e k had its beginnings as a marxist isla...  \n",
              "5  but within a few years the group was waging a ...  \n",
              "8  the m e k s ties with western intelligence dee...  \n",
              "9  funds were covertly passed to a number of diss...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}