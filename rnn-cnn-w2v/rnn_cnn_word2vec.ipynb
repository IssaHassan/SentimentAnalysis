{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "lkgp6rhb9Q6j",
    "outputId": "90137ecf-8336-40a1-9786-8ca5f3337bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "3R7OeLti-vqk",
    "outputId": "8ce4cb31-04be-401a-97c4-ca6179a76362"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n",
      "100%|██████████| 1578612/1578612 [00:40<00:00, 38950.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg its already o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>omgaga im sooo im gunna cry I have been at thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                           CleanText  \n",
       "0                        is so sad for my apl friend  \n",
       "1                      i missed the new moon trailer  \n",
       "2                                  omg its already o  \n",
       "3  omgaga im sooo im gunna cry I have been at thi...  \n",
       "4                i think mi bf is cheating on me t_t  "
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "appos = {\n",
    "  \"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\",\n",
    "  \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
    "  \"haven't\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"i would\",\n",
    "  \"i'd\" : \"i had\", \"i'll\" : \"i will\", \"i'm\" : \"i am\", \"isn't\" : \"is not\", \"it's\" : \"it is\", \"it'll\":\"it will\",\n",
    "  \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\",\n",
    "  \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\",\n",
    "  \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\",\n",
    "  \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \"what'll\" : \"what will\",\n",
    "  \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\",\n",
    "  \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\",\n",
    "  \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\",\n",
    "  \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "  # Remove whitespaces and make strings lowercase\n",
    "  text = text.strip().lower()\n",
    "  words = text.split()\n",
    "  # Nagation handling\n",
    "  reformed = [appos[word] if word in appos else word for word in words]\n",
    "  text = \" \".join(reformed)\n",
    "  pattern = '(@(\\w+))'                # usermention (@username)\n",
    "  pattern += '|(#(\\w+))'              # hashtags (#somehashtag)\n",
    "  pattern += '|([^\\w\\s])'             # emojis 😀\n",
    "  pattern += '|(\\\\w+:\\\\/\\\\/\\\\S+)'     # urls (https://google.com)\n",
    "  pattern += '|(\\d+)'                 # numbers\n",
    "  text = ' '.join(re.sub(pattern, ' ', text).split())\n",
    "  return text\n",
    "\n",
    "tqdm.pandas()\n",
    "data = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/rnn/data/data.csv', error_bad_lines=False)\n",
    "data['CleanText'] = data['SentimentText'].progress_apply(lambda t: clean_text(t))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "EeaEY30SGt78",
    "outputId": "544db066-dd8f-450f-ec2e-6c3b612b1708"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420750,) (157862,) (1420750,) (157862,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['CleanText'], \n",
    "                                                    data['Sentiment'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=data['Sentiment'])\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9A4gR8rLGjC"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoQvOzgzL-pA"
   },
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets, label):\n",
    "  result = []\n",
    "  prefix = label\n",
    "  for i, t in zip(tweets.index, tweets):\n",
    "    result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "  return result\n",
    "\n",
    "all_x = pd.concat([x_train, x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f0jCnT65MfuH",
    "outputId": "4716bec9-454f-4dc4-f8ba-015a0a83b4b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2015741.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "# Continuous Bag Of Words\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "R8TjDALTN-w5",
    "outputId": "a92fe380-b199-457a-b65d-f8e67c3d00eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2023622.85it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 1999302.67it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2032729.29it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2032404.83it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2098025.17it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2040109.24it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2019514.62it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2032640.05it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2037520.20it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2031295.60it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2000554.93it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2024864.28it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2044870.00it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2004972.36it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2103161.94it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2035329.94it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2021596.28it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2036580.13it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 1999712.67it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2018967.79it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2018445.25it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2071715.09it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2042613.50it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2039727.13it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2002624.92it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2055223.72it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2059391.56it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2058589.92it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2029238.09it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2049045.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 4s, sys: 9.15 s, total: 28min 13s\n",
      "Wall time: 15min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "  model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "  model_ug_cbow.alpha -= 0.002\n",
    "  model_ug_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EQQW18vyOM13",
    "outputId": "f68498ce-afc6-43e0-d850-8a4aa8273668"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2047530.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Skip Gram\n",
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "XovOngQWSOjE",
    "outputId": "071e6dbe-4195-40ae-98af-04aaa3b14376"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2016655.65it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 1990392.09it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2070315.22it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2029895.04it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2051058.11it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2068979.31it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2049632.08it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2095968.33it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2061479.25it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2056872.22it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2051952.46it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2057046.67it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2053640.29it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2001390.64it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2064047.86it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2030808.39it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2055994.01it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2088189.36it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2027816.76it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 1991496.63it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2006702.96it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2008606.54it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2111547.90it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2020718.94it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2086570.53it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2057625.84it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2051429.23it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2064296.25it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2043807.68it/s]\n",
      "100%|██████████| 1578612/1578612 [00:00<00:00, 2060097.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49min 45s, sys: 9.23 s, total: 49min 54s\n",
      "Wall time: 26min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "  model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "  model_ug_sg.alpha -= 0.002\n",
    "  model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMnViVuRSbJ0"
   },
   "outputs": [],
   "source": [
    "model_ug_cbow.save('/content/drive/My Drive/SentimentAnalysis/rnn/w2vmodels/w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg.save('/content/drive/My Drive/SentimentAnalysis/rnn/w2vmodels/w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iy87OaQBbhs4",
    "outputId": "b5b898ab-5827-41a1-8720-d0b913a6574e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106091 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SLzGTGY8bkmg",
    "outputId": "c82de89c-942a-4fa9-8c31-f16509df0677"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 80000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data['CleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcq0pYEFcZna"
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkUXpTJPYwX2"
   },
   "outputs": [],
   "source": [
    "# saving tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('/content/drive/My Drive/SentimentAnalysis/rnn/tokenizers/tokenizer-rnn-cnn-w2v.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iA0n3By6dCqY",
    "outputId": "80905eaa-c436-41e4-e465-f13ccac85451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1420750, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LENGTH = 35\n",
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
    "padded_train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlZ4seEWdOag"
   },
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "# maximum number of words kept after tokenization based on their word frequency\n",
    "MAX_NB_WORDS = 80000\n",
    "\n",
    "num_words = MAX_NB_WORDS\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if i >= num_words:\n",
    "    continue\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aR9nfPQegC6"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Conv1D\n",
    "\n",
    "def get_rnn_cnn_model():\n",
    "  embedding_dim = 200\n",
    "  inp = Input(shape=(MAX_LENGTH, ))\n",
    "  x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
    "  x = SpatialDropout1D(0.3)(x)\n",
    "  x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "  x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "  avg_pool = GlobalAveragePooling1D()(x)\n",
    "  max_pool = GlobalMaxPooling1D()(x)\n",
    "  conc = concatenate([avg_pool, max_pool])\n",
    "  outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=outp)\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "aQJz591JfDTq",
    "outputId": "659f3865-c7b0-47cc-d62c-13f36a11a02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1420750 samples, validate on 157862 samples\n",
      "Epoch 1/4\n",
      "1420750/1420750 [==============================] - 1009s 710us/step - loss: 0.4039 - acc: 0.8153 - val_loss: 0.3707 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83615, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-01-0.8362.hdf5\n",
      "Epoch 2/4\n",
      "1420750/1420750 [==============================] - 1002s 706us/step - loss: 0.3637 - acc: 0.8380 - val_loss: 0.3603 - val_acc: 0.8399\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83615 to 0.83987, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-02-0.8399.hdf5\n",
      "Epoch 3/4\n",
      "1420750/1420750 [==============================] - 977s 688us/step - loss: 0.3459 - acc: 0.8477 - val_loss: 0.3604 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.83987 to 0.84191, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-03-0.8419.hdf5\n",
      "Epoch 4/4\n",
      "1420750/1420750 [==============================] - 998s 702us/step - loss: 0.3321 - acc: 0.8550 - val_loss: 0.3605 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84191 to 0.84210, saving model to /content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-04-0.8421.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "rnn_cnn_model = get_rnn_cnn_model()\n",
    "\n",
    "filepath=\"/content/drive/My Drive/SentimentAnalysis/rnn/models/rnn-cnn-w2v-model-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "history = rnn_cnn_model.fit(x=padded_train_sequences, \n",
    "                    y=y_train, \n",
    "                    validation_data=(padded_test_sequences, y_test), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQwo9FZv4nUb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn-cnn-word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
